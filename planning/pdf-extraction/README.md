# PDF Menu Extraction - Option A (Manual Processing)

**Last Updated:** 2025-10-20

## Overview

This document outlines the immediate solution for importing menu data from PDF files when no online ordering platform is available for automated scraping. This approach was created specifically for Chaat Street restaurant (ID: `f2995098-3a86-481e-9cf0-0faf73dcf799`) but serves as a template for similar future scenarios.

### The Challenge

- Restaurant changing menu during onboarding
- New menu only available as PDF (`chaat-street-new-menu.pdf`)
- 13 high-resolution images (6-11MB each) - too large for Cloudwaitress CDN
- No online ordering platform available for automated extraction
- Need to maintain compatibility with existing CSV import workflow

### The Solution

A hybrid manual-automated approach that:
1. Compresses oversized images to CDN-acceptable sizes (~500KB)
2. Manually extracts menu data from PDF into structured CSV
3. Reuses existing CDN images where menu items haven't changed
4. Uploads new compressed images to UploadCare CDN
5. Generates CSV with proper CDN references for automated import

## Documentation Structure

```
planning/pdf-extraction/
â”œâ”€â”€ README.md                        # This file - project overview
â”œâ”€â”€ implementation-roadmap.md        # Phase-by-phase implementation plan
â”œâ”€â”€ architecture.md                  # System design and data flow
â”œâ”€â”€ database-schema.md              # Database tables and relationships
â”œâ”€â”€ service-layer.md                # Service components breakdown
â”œâ”€â”€ image-processing-pipeline.md    # Image compression and upload workflow
â”œâ”€â”€ chaat-street-new-menu.pdf      # Source PDF menu
â”œâ”€â”€ Chaat Street_menu.csv          # Existing menu with CDN references
â””â”€â”€ chaat-street-photos/           # New menu item images (uncompressed)
    â”œâ”€â”€ BEDAI KE ALOO-2.jpg        # 9.7MB
    â”œâ”€â”€ BOMBE KULFI-1.jpg          # 6.3MB
    â”œâ”€â”€ Charred Cabbage Poriyal-2.jpg  # 10MB
    â””â”€â”€ ...                        # 10 more images
```

### Reference Files

```
planning/pdf-extraction/reference-files/
â”œâ”€â”€ uploadcare-integration-plan.md  # UploadCare CDN integration details
â”œâ”€â”€ Multi-Platform-Extraction-Analysis.md
â””â”€â”€ Platform-Expansion-Implementation-Summary.md
```

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Menu      â”‚
â”‚  (Manual Read)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Manual Data Extraction                         â”‚
â”‚  - Compare with existing CSV                    â”‚
â”‚  - Identify new items vs existing items         â”‚
â”‚  - Map images to menu items                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image Processing Pipeline                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Load high-res images (6-11MB each)    â”‚  â”‚
â”‚  â”‚ 2. Compress with Sharp (~500KB target)   â”‚  â”‚
â”‚  â”‚ 3. Upload to UploadCare CDN               â”‚  â”‚
â”‚  â”‚ 4. Receive CDN IDs and URLs               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database Record Creation                       â”‚
â”‚  - Create menu record                           â”‚
â”‚  - Create category records                      â”‚
â”‚  - Create menu_item records                     â”‚
â”‚  - Create item_images records with CDN data     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Enhanced CSV Generation                        â”‚
â”‚  - Include all Cloudwaitress required fields    â”‚
â”‚  - Add CDN references (isCDNImage=TRUE)         â”‚
â”‚  - Include imageCDNID and imageCDNFilename      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Automated Import (Existing Script)             â”‚
â”‚  - Use import-csv-menu.js via Playwright        â”‚
â”‚  - Imports to Pumpd admin dashboard             â”‚
â”‚  - Images already on CDN, references work       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Principles

1. **Reuse Before Upload**: Check existing CSV for items with CDN images before uploading new ones
2. **Compress First**: Always compress images before CDN upload (target ~500KB, max 1MB)
3. **Database First**: Create proper database records before CSV generation
4. **CSV Compatibility**: Generated CSV must match exact format expected by `import-csv-menu.js`
5. **Manual Verification**: Human verification at each critical step to ensure data accuracy

## Existing UI Patterns

This solution does NOT involve UI changes. It leverages:
- Existing UploadCare service (`src/services/uploadcare-service.js`)
- Existing database service (`src/services/database-service.js`)
- Existing CSV generation logic (server.js, lines 4100-4300)
- Existing import script (`scripts/restaurant-registration/import-csv-menu.js`)

## Implementation Location

### New Files to Create

```
scripts/
â””â”€â”€ pdf-menu-processing/
    â”œâ”€â”€ compress-images.js           # Image compression utility
    â”œâ”€â”€ upload-to-cdn.js            # Batch CDN upload with progress
    â”œâ”€â”€ create-menu-from-csv.js     # Database record creation
    â””â”€â”€ merge-csv-references.js     # Merge old CDN refs with new items
```

### Existing Files to Use

- `UberEats-Image-Extractor/src/services/uploadcare-service.js` - CDN upload
- `UberEats-Image-Extractor/src/services/database-service.js` - Database operations
- `scripts/restaurant-registration/import-csv-menu.js` - Final import

## Quick Start Guide

### Prerequisites

1. **Environment Variables** (in `/scripts/.env`):
   ```env
   UPLOADCARE_PUBLIC_KEY=your_key_here
   UPLOADCARE_SECRET_KEY=your_secret_here
   ```

2. **Dependencies Installed**:
   ```bash
   npm install sharp axios uuid csv-parse csv-stringify
   ```

3. **Files Prepared**:
   - PDF menu for reference
   - High-resolution images in a folder
   - Existing CSV with CDN references (if updating menu)

### Usage Steps

```bash
# Step 1: Compress images
cd scripts/pdf-menu-processing
node compress-images.js \
  --input ../../planning/pdf-extraction/chaat-street-photos \
  --output ./compressed-images

# Step 2: Create draft CSV manually
# Open the PDF, compare with existing CSV, create new CSV with new items

# Step 3: Upload new images to CDN
node upload-to-cdn.js \
  --images ./compressed-images \
  --csv ./draft-menu.csv \
  --restaurant-id f2995098-3a86-481e-9cf0-0faf73dcf799

# Step 4: Merge old CDN references
node merge-csv-references.js \
  --old-csv ../../planning/pdf-extraction/Chaat\ Street_menu.csv \
  --new-csv ./draft-menu.csv \
  --output ./final-menu-with-cdn.csv

# Step 5: Create database records
node create-menu-from-csv.js \
  --csv ./final-menu-with-cdn.csv \
  --restaurant-id f2995098-3a86-481e-9cf0-0faf73dcf799

# Step 6: Use existing import script
cd ../restaurant-registration
node import-csv-menu.js \
  --email chaat.street@example.com \
  --csv ../pdf-menu-processing/final-menu-with-cdn.csv
```

## Related Documentation

### Within This Project
- [Implementation Roadmap](./implementation-roadmap.md) - Detailed phase-by-phase plan
- [Architecture](./architecture.md) - System design and data flow diagrams
- [Database Schema](./database-schema.md) - Table structures and relationships
- [Service Layer](./service-layer.md) - Service components breakdown
- [Image Processing Pipeline](./image-processing-pipeline.md) - Image handling workflow

### Reference Files
- `reference-files/uploadcare-integration-plan.md` - UploadCare CDN integration details
- `../../extracted-menus/` - Previously extracted menu CSVs
- `../../scripts/restaurant-registration/import-csv-menu.js` - Import script documentation

### Existing System Documentation
- `../../CLAUDE.md` - Main project documentation
- `../../UberEats-Image-Extractor/` - Extraction system codebase

## Status

**Current Status:** Planning Phase
**Started:** 2025-10-20
**Restaurant:** Chaat Street (ID: f2995098-3a86-481e-9cf0-0faf73dcf799)

### Completed
- âœ… System analysis and architecture review
- âœ… Existing CSV format analysis
- âœ… Image compression requirements determined
- âœ… Database schema review
- âœ… UploadCare service capabilities confirmed

### In Progress
- ğŸ”„ Documentation creation
- ğŸ”„ Implementation planning

### Not Started
- â³ Image compression script
- â³ CDN upload script
- â³ CSV creation and merging scripts
- â³ Database record creation
- â³ Testing and validation

## Next Steps

1. **Complete Documentation** (Current Phase)
   - Finalize implementation roadmap
   - Document architecture details
   - Create database schema reference

2. **Implement Image Processing** (Phase 1)
   - Create compress-images.js script
   - Test compression on sample images
   - Verify CDN size requirements met

3. **Implement CDN Upload** (Phase 2)
   - Create upload-to-cdn.js script
   - Test batch upload functionality
   - Validate CDN responses

4. **CSV Processing** (Phase 3)
   - Manually extract PDF data
   - Create merge-csv-references.js
   - Generate final CSV with all CDN refs

5. **Database & Import** (Phase 4)
   - Create database records
   - Test CSV import
   - Validate in Pumpd dashboard

## Notes

- **Time Sensitivity**: This is an urgent onboarding requirement
- **Manual Steps**: Some manual data extraction required (PDF â†’ CSV)
- **Future Automation**: Consider Firecrawl PDF parser for scalable solution
- **Image Quality**: Balance compression ratio vs visual quality
- **Validation**: Verify each phase before proceeding to next

## Contact

For questions or issues:
- Review existing documentation first
- Check reference files for similar patterns
- Test scripts with sample data before production use
